% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01-LlmPromptConfig-class.R
\name{new_LlmPromptConfig}
\alias{new_LlmPromptConfig}
\title{Create and Manage LLM Prompt Settings}
\usage{
new_LlmPromptConfig(
  prompt_content,
  model,
  max_tokens = 100,
  temperature = 1,
  prompt_role = "user",
  seed = NULL,
  top_p = 1,
  n = 1,
  stop = NULL,
  presence_penalty = 0,
  frequency_penalty = 0,
  logprobs = FALSE
)
}
\arguments{
\item{prompt_content}{character string containing the primary instruction or query for the model. This serves as the main input to the LLM.}

\item{model}{Character string specifying the model to use (e.g., `'gpt-4.1'` for OpenAI or `'deepseek-chat'` for DeepSeek).
 To retrieve a list of valid models for each LLM, use the \code{get_llm_models()} method

See the following documentation for valid models:
- \href{https://platform.openai.com/docs/models}{OpenAI model list}
- \href{https://api-docs.deepseek.com/api/list-models}{DeepSeek model list}}

\item{max_tokens}{numeric (default: 100) defining the maximum number of tokens to be generated in the response.}

\item{temperature}{numeric (default: 1.0) controlling randomness in responses. Lower values (e.g., 0.2) make responses deterministic, while higher values (e.g., 1.5) increase creativity.}

\item{prompt_role}{character (default: 'user') specifying the role of the message. Common values include 'system', 'assistant', and 'user'.}

\item{seed}{numeric (optional) for controlling reproducibility. If NULL, no seed is set.}

\item{top_p}{numeric (default: 1) alternative to temperature, specifying nucleus sampling probability. A value of 0.1 considers only the top 10\% probability mass.}

\item{n}{numeric (default: 1) defining the number of responses to generate per request. If temperature is 0, n is automatically set to 1.}

\item{stop}{character or character vector (default: NULL) defining stop sequences for response termination. Up to 4 sequences can be specified.}

\item{presence_penalty}{numeric (default: 0) between -2.0 and +2.0, influencing model inclination to introduce new topics.}

\item{frequency_penalty}{numeric (default: 0) between -2.0 and +2.0, influencing model tendency to repeat words or phrases.}

\item{logprobs}{boolean (default: FALSE) specifying whether to return log probabilities for output tokens.}
}
\value{
An object of class LlmPromptConfig, containing all specified parameters in a structured format.
}
\description{
The new_LlmPromptConfig() function constructs an S3 object that stores the parameters required for
making requests to Large Language Models (LLMs) such as OpenAI's GPT models and DeepSeek models.
}
\examples{
\dontrun{
# Retrieve available models
api <- new_RemoteLlmApi(api_key_path = "path/to/openai_key.txt", provider = "OpenAI")
models <- get_llm_models(api)
}

# Create a parameter object for OpenAI GPT-4.1
params <- new_LlmPromptConfig(
  prompt_content = 'Explain entropy in simple terms.',
  model = 'gpt-4.1',
  temperature = 0.7,
  max_tokens = 150
)

# Create a parameter object for DeepSeek
params <- new_LlmPromptConfig(
  prompt_content = 'What are three innovative AI research topics?',
  model = 'deepseek-chat',
  temperature = 0.9,
  n = 3
)

# Print the parameter object
print(params)
}
