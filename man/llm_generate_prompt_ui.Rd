% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/04-llm_generate_prompt-module.R
\name{llm_generate_prompt_ui}
\alias{llm_generate_prompt_ui}
\title{LLM Prompt Generator UI Module}
\usage{
llm_generate_prompt_ui(
  id,
  prompt_beginning = "",
  prompt_placeholder = "Ask me anything...",
  theme = "xcode",
  outputResponse = FALSE
)
}
\arguments{
\item{id}{A unique string identifying the module namespace.}

\item{prompt_beginning}{Optional character string shown as a prefix label before the prompt input. Default is `""`.}

\item{prompt_placeholder}{Placeholder text shown in the prompt input field. Default is `"Ask me anything..."`.}

\item{theme}{Editor theme for the ACE input. Defaults to `"xcode"`.}

\item{outputResponse}{Logical; whether to show the generated response output below the input UI. Default is `FALSE`.}
}
\value{
A UI definition (tagList) that can be included in a Shiny app.
}
\description{
Provides a user interface to enter a prompt, configure LLM API access, and optionally display the generated text output.
}
\details{
This module renders the following elements:
- LLM API configuration UI (via `llm_api_ui`)
- Prompt configuration UI (via `llm_prompt_config_ui`)
- An ACE code editor for prompt input
- A "Generate Text" button with status messaging
- Optional display of the generated response (controlled by `outputResponse`)
}
\examples{
ui <- fluidPage(
  shinyjs::useShinyjs(),
  llm_generate_prompt_ui("my_prompt")
)

server <- function(input, output, session) {
  llm_generate_prompt_server("my_prompt")
}

shinyApp(ui, server)

}
\seealso{
\code{\link{llm_generate_prompt_server}} for the server-side logic.
}
