% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/04-llm_generate_prompt-module.R
\name{llm_generate_prompt_server}
\alias{llm_generate_prompt_server}
\title{LLM Prompt Generator Server Module}
\usage{
llm_generate_prompt_server(
  id,
  autoCompleteList = reactive(NULL),
  no_internet = NULL,
  excludePattern = ""
)
}
\arguments{
\item{id}{A string specifying the module namespace, matching the `id` used in `llm_generate_prompt_ui`.}

\item{autoCompleteList}{A reactive list.}

\item{no_internet}{logical}

\item{excludePattern}{character, e.g. "babbage|curie|dall-e|davinci|text-embedding|tts|whisper"}
}
\value{
A reactive value (`reactiveVal`) containing the `LlmResponse` object returned from the LLM API.
}
\description{
Server-side logic for handling prompt input, LLM API interaction, response handling, and error/status display.
}
\details{
The server module:
- Initializes the LLM API and prompt configuration modules.
- Enables or disables the "Generate Text" button based on configuration readiness.
- On click, sends a prompt to the API and stores the result in a `reactiveVal`.
- Uses `statusMessageServer()` to provide feedback on the response generation status.
- Renders the LLM response using `renderPrint()` if UI is configured to do so.

It depends on these additional server modules:
- `llm_api_server()` for managing API key and connection
- `llm_prompt_config_server()` for prompt tuning options
- `statusMessageServer()` to show status messages like success, warning, or error
}
\seealso{
\code{\link{llm_generate_prompt_ui}} for the UI component.
}
