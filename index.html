<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>R Interface for Large Language Model APIs â€¢ llmModule</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="R Interface for Large Language Model APIs">
<meta name="description" content="Provides a structured interface for interacting with Large Language Model (LLM) APIs such as OpenAI and DeepSeek. Includes functions for sending requests and extracting structured responses. This package simplifies API interactions and ensures validation of API keys and request formats.">
<meta property="og:description" content="Provides a structured interface for interacting with Large Language Model (LLM) APIs such as OpenAI and DeepSeek. Includes functions for sending requests and extracting structured responses. This package simplifies API interactions and ensures validation of API keys and request formats.">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">llmModule</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">25.06.5</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header"><h1 id="llmmodule-development-version">llmModule (development version)<a class="anchor" aria-label="anchor" href="#llmmodule-development-version"></a>
</h1></div>
<!-- badges: start -->

<p><code>llmModule</code> provides a structured and extensible R interface to interact with both remote (e.g., OpenAI, DeepSeek) and local (via Ollama) Large Language Model (LLM) APIs. It simplifies key workflows such as model selection, prompt configuration, and request handling through a consistent object-oriented interface.</p>
<p><code>llmModule</code> provides a structured R interface for working with Large Language Model (LLM) APIs, including <a href="https://platform.openai.com" class="external-link">OpenAI</a> and <a href="https://platform.deepseek.com" class="external-link">DeepSeek</a>.</p>
<p>It simplifies interactions with chat-based LLMs by offering methods and S3 classes for:</p>
<ul>
<li>API key management and validation</li>
<li>Prompt configuration</li>
<li>Sending chat prompts</li>
<li>Extracting responses</li>
</ul>
<div class="section level2">
<h2 id="id_-features">ðŸš€ Features<a class="anchor" aria-label="anchor" href="#id_-features"></a>
</h2>
<ul>
<li>Modular, object-oriented interface using S3 classes:
<ul>
<li>
<code>RemoteLlmApi</code> for remote providers (OpenAI, DeepSeek)</li>
<li>
<code>LocalLlmApi</code> for local Ollama servers</li>
<li>
<code>LlmPromptConfig</code> to configure prompt messages and parameters</li>
<li>
<code>LlmResponse</code> for structured handling of responses</li>
</ul>
</li>
<li>Comprehensive API validation:
<ul>
<li>Validates API key format, provider match, and key functionality via test request</li>
<li>Clear error reporting with automatic suggestion of likely provider mismatches</li>
</ul>
</li>
<li>Local model support (via <a href="https://ollama.com" class="external-link">Ollama</a>):
<ul>
<li>Allows to pull new models</li>
<li>Allows exclusion of deprecated or irrelevant models via <code>exclude_pattern</code>
</li>
</ul>
</li>
<li>Unified method interface:
<ul>
<li>
<code><a href="reference/get_llm_models.html">get_llm_models()</a></code> to fetch available models</li>
<li>
<code><a href="reference/send_prompt.html">send_prompt()</a></code> to submit prompts and retrieve responses</li>
</ul>
</li>
<li>Optional Docker integration for local deployment (see below)</li>
</ul>
<hr>
</div>
<div class="section level2">
<h2 id="id_-quick-example">ðŸ§ª Quick Example<a class="anchor" aria-label="anchor" href="#id_-quick-example"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create an LLM API object</span></span>
<span><span class="va">api</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/new_RemoteLlmApi.html">new_RemoteLlmApi</a></span><span class="op">(</span><span class="st">"~/.secrets/openai.txt"</span>, provider <span class="op">=</span> <span class="st">"OpenAI"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set up a prompt</span></span>
<span><span class="va">prompt</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/new_LlmPromptConfig.html">new_LlmPromptConfig</a></span><span class="op">(</span></span>
<span>      model <span class="op">=</span> <span class="st">"gpt-4.1"</span>,</span>
<span>      prompt_content <span class="op">=</span> <span class="st">"What's the capital of Italy?"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Send the prompt</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/send_prompt.html">send_prompt</a></span><span class="op">(</span><span class="va">api</span>, <span class="va">prompt</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract the assistant's reply</span></span>
<span><span class="va">result</span><span class="op">$</span><span class="va">choices</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">message</span><span class="op">$</span><span class="va">content</span></span></code></pre></div>
<hr>
</div>
<div class="section level2">
<h2 id="id_-docker-installation-recommended">ðŸ“¦ Docker Installation (recommended)<a class="anchor" aria-label="anchor" href="#id_-docker-installation-recommended"></a>
</h2>
<p>Run this app in your browser with just one command! The Docker setup includes all components â€” the <code>llmModule</code> Shiny frontend and the <code>ollama</code> backend for local LLM model serving â€” no manual setup required.</p>
<div class="section level3">
<h3 id="id_-1-install-the-software-docker">âœ… 1. Install the software Docker<a class="anchor" aria-label="anchor" href="#id_-1-install-the-software-docker"></a>
</h3>
<p>Download installation files from one of the links below and follow installation instructions:</p>
<ul>
<li><a href="https://docs.docker.com/desktop/windows/install/" class="external-link">Windows</a></li>
<li><a href="https://docs.docker.com/desktop/install/mac-install/" class="external-link">MacOS</a></li>
<li><a href="https://docs.docker.com/desktop/install/linux-install/" class="external-link">Linux</a></li>
</ul>
<p>After Docker is installed you can pull &amp; run the app manually.</p>
</div>
<div class="section level3">
<h3 id="id_-2-run-the-app-with-docker-compose">ðŸš€ 2. Run the App with Docker Compose<a class="anchor" aria-label="anchor" href="#id_-2-run-the-app-with-docker-compose"></a>
</h3>
<p><strong>Open a terminal (command line):</strong></p>
<ul>
<li>Windows command line:
<ol style="list-style-type: decimal">
<li>Open the Start menu or press the <code>Windows key</code> + <code>R</code>;</li>
<li>Type cmd or cmd.exe in the Run command box;</li>
<li>Press Enter.</li>
</ol>
</li>
<li>MacOS: open the Terminal app.</li>
<li>Linux: most Linux systems use the same default keyboard shortcut to start the command line: <code>Ctrl</code>-<code>Alt</code>-<code>T</code> or <code>Super</code>-<code>T</code>
</li>
</ul>
<p>To start the app you need the <a href="https://github.com/Pandora-IsoMemo/llmModule/blob/main/docker-compose.yml" class="external-link">docker-compose.yaml</a> of this Repository. You can either:</p>
<p><strong>Run directly without cloning the repo:</strong></p>
<pre><code>curl -sL https://raw.githubusercontent.com/Pandora-IsoMemo/llmModule/refs/heads/main/docker-compose.yml | docker compose -f - up</code></pre>
<p><strong>OR: Clone the repository and run in the project directory:</strong></p>
<pre><code>git clone https://github.com/Pandora-IsoMemo/llmModule.git
cd llmModule
docker compose up</code></pre>
<p>These commands will:</p>
<ol style="list-style-type: decimal">
<li>The first time you run this, it will download the necessary Docker images for
<ul>
<li>
<code>ollama</code> (for model serving and its REST API) and</li>
<li>the <code>llm-module</code> (the Shiny web frontend that controls Ollama and can also interact with other LLM APIs like OpenAI, Deepseek).</li>
</ul>
</li>
<li>After images are pulled, a Docker network and a Docker volume will be created, and both container will start.</li>
<li>The <code>llm-module</code> container hosts the application, which you can access in your web browser at <code>http://127.0.0.1:3838/</code>.</li>
</ol>
</div>
<div class="section level3">
<h3 id="id_-3-run-the-app-with-a-custom-ollama-models-path-optional">ðŸš€ 3. Run the app with a custom Ollama models path (optional)<a class="anchor" aria-label="anchor" href="#id_-3-run-the-app-with-a-custom-ollama-models-path-optional"></a>
</h3>
<p>To use your own pre-downloaded Ollama models, specify a custom path by setting the <code>OLLAMA_LOCAL_MODELS_PATH</code> environment variable.</p>
<p>This requires cloning the repository and running Docker Compose from the project directory.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="va">OLLAMA_LOCAL_MODELS_PATH</span><span class="op">=</span>/path/to/your/models <span class="ex">docker</span> compose up</span></code></pre></div>
<p>Default locations for Ollama models:</p>
<ul>
<li>linux: <code>/usr/share/ollama/.ollama</code>
</li>
<li>macOS: <code>~/.ollama</code>
</li>
<li>windows: <code>C:\\Users\\&lt;username&gt;\\.ollama</code>
</li>
</ul>
<p>This will mount your local models into the container for faster startup and persistent access.</p>
<hr>
</div>
</div>
<div class="section level2">
<h2 id="notes-for-developers--local-testing">Notes for developers â€” local testing<a class="anchor" aria-label="anchor" href="#notes-for-developers--local-testing"></a>
</h2>
<p>To build and run the app locally:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="ex">docker</span> compose up <span class="at">--build</span></span></code></pre></div>
<p>Use <code>--build</code> if:</p>
<ul>
<li>You made changes to source code or Dockerfiles,</li>
<li>Or youâ€™re testing a fresh environment.</li>
</ul>
<p>To run with a custom models path:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="va">OLLAMA_LOCAL_MODELS_PATH</span><span class="op">=&lt;</span>/path/to/your/models<span class="op">&gt;</span> docker <span class="ex">compose</span> up <span class="at">--build</span></span></code></pre></div>
<p><em>Tip:</em> Use <code>docker compose down</code> to stop and clean up the containers when done.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing llmModule</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Ricardo Fernandes <br><small class="roles"> Author, maintainer </small>   </li>
<li>Antonia Runge <br><small class="roles"> Author </small>   </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/Pandora-IsoMemo/llmModule/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/Pandora-IsoMemo/llmModule/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Ricardo Fernandes, Antonia Runge.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
